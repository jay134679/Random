{
 "metadata": {
  "name": "",
  "signature": "sha256:91a9aa9aedc3c710cca1eb1c1466a10d3f083fc83d51f7ddbdaafc127c53c6fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "HW3 - Maya Rotmensch"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Introduction "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# nothing to do "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. The Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pickle\n",
      "import load\n",
      "from collections import Counter\n",
      "from collections import defaultdict\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import pickle\n",
      "import random\n",
      "\n",
      "'''\n",
      "Note: No obligation to use this code, though you may if you like.  Skeleton code is just a hint for people who are not familiar with text processing in python. \n",
      "It is not necessary to follow. \n",
      "'''\n",
      "\n",
      "\n",
      "def folder_list(path,label):\n",
      "    '''\n",
      "    PARAMETER PATH IS THE PATH OF YOUR LOCAL FOLDER\n",
      "    '''\n",
      "    filelist = os.listdir(path)\n",
      "    review = []\n",
      "    y = []\n",
      "    for infile in filelist:\n",
      "        file = os.path.join(path,infile)\n",
      "        r = read_data(file)\n",
      "        #r.append(label)\n",
      "        y.append(label)\n",
      "        review.append(r)\n",
      "    return review, y\n",
      "\n",
      "def read_data(file):\n",
      "    '''\n",
      "    Read each file into a list of strings. \n",
      "    Example:\n",
      "    [\"it's\", 'a', 'curious', 'thing', \"i've\", 'found', 'that', 'when', 'willis', 'is', 'not', 'called', 'on', \n",
      "    ...'to', 'carry', 'the', 'whole', 'movie', \"he's\", 'much', 'better', 'and', 'so', 'is', 'the', 'movie']\n",
      "    '''\n",
      "    f = open(file)\n",
      "    lines = f.read().split(' ')\n",
      "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
      "    words = map(lambda Element: Element.translate(None, symbols).strip(), lines)\n",
      "    words = filter(None, words)\n",
      "    return words\n",
      "\n",
      "###############################################\n",
      "######## YOUR CODE STARTS FROM HERE. ##########\n",
      "###############################################\n",
      "\n",
      "def shuffle_data():\n",
      "    '''\n",
      "    pos_path is where you save positive review data.\n",
      "    neg_path is where you save negative review data.\n",
      "    '''\n",
      "    pos_path = \"/Users/mayarotmensch/Documents/Machine Learning/hw3-sentiment/data/neg\"\n",
      "    neg_path = \"/Users/mayarotmensch/Documents/Machine Learning/hw3-sentiment/data/pos\"\n",
      "\n",
      "    #print \"erfydhcb\"\n",
      "\n",
      "    pos_review, y_pos = folder_list(pos_path,1)\n",
      "    neg_review, y_neg = folder_list(neg_path,-1)\n",
      "\n",
      "    review = pos_review + neg_review\n",
      "    y = y_pos + y_neg\n",
      "    \n",
      "    # shuffle\n",
      "    combined = list(zip(review, y))\n",
      "    random.shuffle(combined)\n",
      "    #review, y = zip(*combined)\n",
      "    \n",
      "    pickle.dump(combined, open(\"reviews_in_memory.pkl\", 'wb'))\n",
      "    #pickle.dump(y, open(\"reviews_in_memory.pkl\", 'wb'))\n",
      "    #return review, y\n",
      "    \n",
      "'''\n",
      "Now you have read all the files into list 'review' and it has been shuffled.\n",
      "Save your shuffled result by pickle.\n",
      "*Pickle is a useful module to serialize a python object structure. \n",
      "*Check it out. https://wiki.python.org/moin/UsingPickle\n",
      "'''\n",
      " \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "\"\\nNow you have read all the files into list 'review' and it has been shuffled.\\nSave your shuffled result by pickle.\\n*Pickle is a useful module to serialize a python object structure. \\n*Check it out. https://wiki.python.org/moin/UsingPickle\\n\""
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# shuffle_data() #<- only use if you want a new shuffle order. otherwise just load from file for faster run times\n",
      "\n",
      "combined = pickle.load(open(\"reviews_in_memory.pkl\", 'rb'))\n",
      "review, y = zip(*combined)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_test_split(X,y, train = 0.75 , test = 0.25):\n",
      "    ''' function assumes that the list is pre-shuffled'''\n",
      "    train_portion = int(len(X)*0.75)\n",
      "    X_train, y_train = X[:train_portion], y[:train_portion]\n",
      "    X_test, y_test = X[train_portion:], y[train_portion:]\n",
      "    return X_train,y_train, X_test, y_test\n",
      "    \n",
      "X_train,y_train, X_test, y_test = train_test_split(review, y, train = 0.75 , test = 0.25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. Sparse Representation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bag_of_words(X):\n",
      "    ''' bag of words'''\n",
      "    X_dict = []\n",
      "    for review in X:\n",
      "        cnt = Counter()\n",
      "        for word in review:\n",
      "            cnt[word] = +1\n",
      "        X_dict.append(dict(cnt))\n",
      "    return X_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 229
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_dict = bag_of_words(X_train)\n",
      "X_test_dict = bag_of_words(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 230
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Taken from http://web.stanford.edu/class/cs221/ Assignment #2 Support Code\n",
      "\n",
      "def dotProduct(d1, d2):\n",
      "    \"\"\"\n",
      "    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n",
      "    @param dict d2: same as d1\n",
      "    @return float: the dot product between d1 and d2\n",
      "    \"\"\"\n",
      "    if len(d1) < len(d2):\n",
      "        return dotProduct(d2, d1)\n",
      "    else:\n",
      "        return sum(d1.get(f, 0) * v for f, v in d2.items())\n",
      "\n",
      "def increment(d1, scale, d2):\n",
      "    \"\"\"\n",
      "    Implements d1 += scale * d2 for sparse vectors.\n",
      "    @param dict d1: the feature vector which is mutated.\n",
      "    @param float scale\n",
      "    @param dict d2: a feature vector.\n",
      "    \"\"\"\n",
      "    for f, v in d2.items():\n",
      "        d1[f] = d1.get(f, 0) + v * scale\n",
      "\n",
      "def return_increment(d1, scale, d2): ## only new function added to this section. Like increment, only doesn't change in place.\n",
      "    \"\"\"\n",
      "    Implements d1 += scale * d2 for sparse vectors.\n",
      "    @param dict d1: the feature vector which is mutated.\n",
      "    @param float scale\n",
      "    @param dict d2: a feature vector.\n",
      "    \"\"\"\n",
      "    copy = d1.copy()\n",
      "    for f, v in d2.items():\n",
      "        copy[f] = copy.get(f, 0) + v * scale\n",
      "    return copy\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 231
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''def stochastic_grad_checker(Xj, yj, w, epsilon=0.01, tolerance=1e-3, Lambda = 0.0):\n",
      "    \"\"\" \n",
      "    Args:\n",
      "        X - a single instance in the data set (vector).\n",
      "        y - the corresponding target value for the given X vector. (scalar).\n",
      "        row - the index number of  for which we \n",
      "        theta - current theta\n",
      "        objective_func, gradient_func, epsilon=0.01, tolerance=1e-4\n",
      "    \"\"\"\n",
      "    \n",
      "    #Since we\u2019ll be using it for stochastic methods, it should take a single (x, y) pair, rather than the entire dataset. \n",
      "    if (1- yj*dotProduct(Xj, w) ) >0:\n",
      "        true_grad = return_increment({}, Lambda, w)  +  return_increment({}, yj, Xj)\n",
      "    else:\n",
      "        true_grad = return_increment({}, Lambda, w)  \n",
      "\n",
      "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
      "        \n",
      "    for i in range(len(Xj)):\n",
      "        direction = np.zeros(num_features)\n",
      "        direction[i] = 1         \n",
      "        \n",
      "        approx_grad[i] = (SVM_loss(Xj, yj, return_increment(theta, epsilon, direction)) - SVM_loss(Xj, yj, return_increment(theta, -epsilon, direction)))/(2*epsilon)\n",
      "    return np.sqrt(dotProduct(true_gradient - approx_grad,true_gradient - approx_grad)) < epsilon\n",
      "'''\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 232,
       "text": [
        "'def stochastic_grad_checker(Xj, yj, w, epsilon=0.01, tolerance=1e-3, Lambda = 0.0):\\n    \"\"\" \\n    Args:\\n        X - a single instance in the data set (vector).\\n        y - the corresponding target value for the given X vector. (scalar).\\n        row - the index number of  for which we \\n        theta - current theta\\n        objective_func, gradient_func, epsilon=0.01, tolerance=1e-4\\n    \"\"\"\\n    \\n    #Since we\\xe2\\x80\\x99ll be using it for stochastic methods, it should take a single (x, y) pair, rather than the entire dataset. \\n    if (1- yj*dotProduct(Xj, w) ) >0:\\n        true_grad = return_increment({}, Lambda, w)  +  return_increment({}, yj, Xj)\\n    else:\\n        true_grad = return_increment({}, Lambda, w)  \\n\\n    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\\n        \\n    for i in range(len(Xj)):\\n        direction = np.zeros(num_features)\\n        direction[i] = 1         \\n        \\n        approx_grad[i] = (SVM_loss(Xj, yj, return_increment(theta, epsilon, direction)) - SVM_loss(Xj, yj, return_increment(theta, -epsilon, direction)))/(2*epsilon)\\n    return np.sqrt(dotProduct(true_gradient - approx_grad,true_gradient - approx_grad)) < epsilon\\n'"
       ]
      }
     ],
     "prompt_number": 232
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Support Vector Machine (Via Pegasos)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. \n",
      "\n",
      "We start with out objective function:\n",
      "\n",
      "$$ Z =   \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{m} \\sum_{i=1}^{m} max\\{0,1-y_iw^TX_i\\} $$\n",
      "\n",
      "alternatively this objective function can be written as:\n",
      "$$ Z = \n",
      "\\begin{cases}\n",
      " \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{m} \\sum_{i=1}^{m} 1-y_iw^TX_i \\ \\ for \\ \\  1-y_iw^TX_i > 0 \\\\\n",
      " \\frac{\\lambda}{2} \\|w\\|^2  \\ \\ \\ \\ for \\ \\  1-y_iw^TX_i < 0 \\\\\n",
      "\\end{cases}\n",
      "$$\n",
      "\n",
      "\n",
      "$$\\frac{\\partial Z}{\\partial w}  = \n",
      "\\begin{cases}\n",
      " \\lambda w^T + \\frac{1}{m} \\sum_{i=1}^{m} y_iX_i \\ \\ for \\ \\  1-y_iw^Tx_i > 0 \\\\\n",
      " \\lambda w^T  \\ \\ \\ \\ for \\ \\  1-y_iw^TX_i < 0 \\\\\n",
      "\\end{cases}$$\n",
      "\n",
      "Because we are trying to find the formula for stocastic updating then we are only looking at one $X_i$, which eliminates the sum and the $\\frac{1}{m}$.\n",
      "\n",
      "$$\\frac{\\partial Z_i}{\\partial w}  = \n",
      "\\begin{cases}\n",
      " \\lambda w^T +  y_iX_i \\ , \\ for \\ i \\ \\ s.t \\ \\  1-y_iw^Tx_i > 0 \\\\\n",
      " \\lambda w^T  \\ \\ \\ , \\ for \\  i \\ \\ s.t \\ \\  1-y_iw^TX_i < 0 \\\\\n",
      "\\end{cases}$$\n",
      "\n",
      "Finally, looking at formula to update $w$ for a given iteration $t$ , where stepsize is denoted by $\\eta$, we get:\n",
      "$$ w_{t+1} =\n",
      "\\begin{cases} \n",
      "w_t - \\eta_t( \\lambda w_t +  (y_iX_i)^T )\\ , \\ for \\ i \\ \\ s.t \\ \\  1-y_iw^Tx_i > 0 \\\\\n",
      "w_t - \\eta_t( \\lambda w_t ) \\ , \\ for \\ i \\ \\ s.t \\ \\  1-y_iw^Tx_i < 0 \\\\\n",
      "\\end{cases}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_mini = X_train_dict[:100]\n",
      "y_mini = y_train[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 233
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## SVM loss and Zero-one-loss to aid with later calculations\n",
      "def zero_one_loss(X,y,w):\n",
      "    incorrect = 0\n",
      "    for j, xj in enumerate(X):\n",
      "        if y[j]*dotProduct(xj, w) < 0: #incorrect classification\n",
      "            incorrect +=1\n",
      "            \n",
      "    return float(incorrect)/len(X)\n",
      "zero_one_loss(X_mini,y_mini,w = {})   \n",
      "\n",
      "\n",
      "def SVM_loss(X,y,w, Lambda = 0.0):\n",
      "    term_1 = (float(Lambda)/2)*dotProduct(w,w)\n",
      "    \n",
      "    term_2 = 0.0\n",
      "    \n",
      "    for j, xj in enumerate(X):\n",
      "        temp = 1- y[j]*dotProduct(xj, w)\n",
      "        if temp > 0: #\n",
      "            term_2 += temp\n",
      "    #print term_2\n",
      "    return term_1 +term_2/len(X)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Pegasos_algorithm(X, y, Lambda = 1, max_epochs = 1000):\n",
      "    w = Counter()\n",
      "    instances = len(X)\n",
      "    t = 0.0  # iteration counter. decides which instance we are looking at\n",
      "    epoch = 0\n",
      "    assert Lambda >0;\n",
      "    convergence = False\n",
      "    \n",
      "    while (epoch <= max_epochs):\n",
      "        for j in range(instances):\n",
      "            t += 1\n",
      "            stepsize = 1/float(t*Lambda)\n",
      "\n",
      "            # for both \n",
      "            increment(w, (-stepsize*Lambda), w) \n",
      "\n",
      "            if y[j]*dotProduct(w, X[j]) <1:\n",
      "                increment(w, (stepsize*y[j]), X[j])\n",
      "\n",
      "        print \"epoch\", epoch, \" training\", zero_one_loss(X,y,w) \n",
      "        epoch +=1\n",
      "    return w\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 234
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "W_2 = Pegasos_algorithm(X_mini, y_mini, max_epochs = 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "epoch 0  training 0.19\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1  training 0.01\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2  training 0.01\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 9  training 0.0\n",
        "epoch"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10  training 0.0\n"
       ]
      }
     ],
     "prompt_number": 235
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## zero_one_loss has been introduced earlier so we could make use of it in the pegasos algorithm.\n",
      "## Here is an example of its results.\n",
      "zero_one_loss(X_test_dict, y_test,W_2)\n",
      "\n",
      "## note that : \n",
      "# a) the function returns the fraction of incorrect classifications.\n",
      "# b) we're currently doing very badly because we are only training on 100 instances from the full dataset."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 236,
       "text": [
        "0.248"
       ]
      }
     ],
     "prompt_number": 236
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## In order to speed up the search for Lambda, we first must vectorize the pegasos algorithm using the decomposition of\n",
      "## w into magnitute and direction, as explained in the prof's suggested papaer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Vectorized_Pegasos_algorithm(X, y, Lambda = 1.0, max_epochs = 1000):\n",
      "    W = {}\n",
      "    s = 1.0\n",
      "    instances = len(X)\n",
      "    t = 1.0  # iteration counter. decides which instance we are looking at\n",
      "    epoch = 0\n",
      "    \n",
      "    \n",
      "    assert Lambda >0;\n",
      "    \n",
      "    while (epoch <= max_epochs):\n",
      "        for j in range(instances):\n",
      "            t += 1\n",
      "            stepsize = 1/float(t*Lambda)\n",
      "\n",
      "            # for both \n",
      "            s = (1 - stepsize*Lambda)*s\n",
      "                        \n",
      "            if (s*y[j])*dotProduct(W, X[j]) <1:\n",
      "                increment(W, ((stepsize*y[j])/s), X[j])\n",
      "                \n",
      "        \n",
      "        w = return_increment({}, s, W)\n",
      "        #zero_one_loss(X,y,w)\n",
      "        #print SVM_loss(X,y,w, Lambda)\n",
      "        epoch +=1\n",
      "    \n",
      "    return w\n",
      "\n",
      "#Vectorized_Pegasos_algorithm(X_mini, y_mini, max_epochs = 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 237
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Lambda_range1 = np.linspace(0.01, 10, 10 )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 238
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def optimize_lambda(X_train, y_train, X_test, y_test, Lambda_range):\n",
      "    Lambda_opt = 0.0\n",
      "    test_loss = 1000.0\n",
      "    for Lambda in Lambda_range:\n",
      "        optimal_w = Vectorized_Pegasos_algorithm(X_train, y_train, Lambda=Lambda, max_epochs = 30)        \n",
      "        new_loss = zero_one_loss(X_test, y_test, optimal_w)\n",
      "        \n",
      "        print \"Lambda: \" , Lambda\n",
      "        print \"loss : \", SVM_loss(X_test ,y_test, optimal_w, Lambda = 0)\n",
      "        print \"zero-one loss : \", new_loss\n",
      "        print \"************\"\n",
      "        if new_loss < test_loss:\n",
      "            test_loss = new_loss\n",
      "            Lambda_opt = Lambda  \n",
      "    return Lambda_opt\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Lambda_opt = optimize_lambda(X_train_dict,y_train, X_test_dict, y_test, Lambda_range = Lambda_range1)\n",
      "print Lambda_opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lambda:  0.01\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.421371239328\n",
        "zero-one loss :  0.168\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.12\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.690204567029\n",
        "zero-one loss :  0.164\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.23\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.819965779221\n",
        "zero-one loss :  0.254\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.34\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.880616462179\n",
        "zero-one loss :  0.282\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.45\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.910466277275\n",
        "zero-one loss :  0.284\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.56\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.928375347076\n",
        "zero-one loss :  0.288\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.67\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.940280073937\n",
        "zero-one loss :  0.288\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7.78\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.948778214074\n",
        "zero-one loss :  0.284\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.89\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.955173735151\n",
        "zero-one loss :  0.284\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10.0\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.96014166147\n",
        "zero-one loss :  0.284\n",
        "************\n",
        "1.12\n"
       ]
      }
     ],
     "prompt_number": 240
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Lambda_range2 = np.linspace(0.01, 2, 10 )\n",
      "\n",
      "Lambda_opt = optimize_lambda(X_train_dict,y_train, X_test_dict, y_test, Lambda_range = Lambda_range2)\n",
      "print Lambda_opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lambda:  0.01\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.421371239328\n",
        "zero-one loss :  0.168\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.231111111111\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.506245909517\n",
        "zero-one loss :  0.144\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.452222222222\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.55759871091\n",
        "zero-one loss :  0.144\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.673333333333\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.610167761749\n",
        "zero-one loss :  0.156\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.894444444444\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.652590792611\n",
        "zero-one loss :  0.16\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.11555555556\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.689660362357\n",
        "zero-one loss :  0.162\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.33666666667\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.723183041622\n",
        "zero-one loss :  0.182\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.55777777778\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.752403446787\n",
        "zero-one loss :  0.21\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.77888888889\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.776748970373\n",
        "zero-one loss :  0.22\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.0\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.79923352186\n",
        "zero-one loss :  0.242\n",
        "************\n",
        "0.231111111111\n"
       ]
      }
     ],
     "prompt_number": 241
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "0.23111111111111113"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_buckets = Vectorized_Pegasos_algorithm(X_train_dict, y_train, Lambda=Lambda_opt, max_epochs = 30)   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def confidence_loss(X,y,w):\n",
      "    D = defaultdict(list)\n",
      "    for j, xj in enumerate(X):\n",
      "        y_predict = dotProduct(xj,w)\n",
      "        key = round(y_predict,1)\n",
      "        if y[j]*y_predict < 0:\n",
      "            D[key].append(1)\n",
      "        else: \n",
      "            D[key].append(0)\n",
      "    return D\n",
      "        \n",
      "        \n",
      "D = confidence_loss(X_test_dict,y_test,w_buckets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = defaultdict(float)\n",
      "for key in sorted(D.keys()):\n",
      "     print \" For rounded values :  \", key, \"fraction of incorrect : \", float(np.sum(D[key] ))/ len(D[key])\n",
      "     res[key] = float(np.sum(D[key] ))/ len(D[key])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " For rounded values :   -3.1 fraction of incorrect :  0.0\n",
        " For rounded values :   -2.3 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.9 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.7 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.5 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.4 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.3 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.2 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.1 fraction of incorrect :  0.0\n",
        " For rounded values :   -1.0 fraction of incorrect :  0.0\n",
        " For rounded values :   -0.9 fraction of incorrect :  0.0\n",
        " For rounded values :   -0.8 fraction of incorrect :  0.117647058824\n",
        " For rounded values :   -0.7 fraction of incorrect :  0.0\n",
        " For rounded values :   -0.6 fraction of incorrect :  0.0\n",
        " For rounded values :   -0.5 fraction of incorrect :  0.0526315789474\n",
        " For rounded values :   -0.4 fraction of incorrect :  0.115384615385\n",
        " For rounded values :   -0.3 fraction of incorrect :  0.25\n",
        " For rounded values :   -0.2 fraction of incorrect :  0.416666666667\n",
        " For rounded values :   -0.1 fraction of incorrect :  0.307692307692\n",
        " For rounded values :   -0.0 fraction of incorrect :  0.5\n",
        " For rounded values :   0.1 fraction of incorrect :  0.428571428571\n",
        " For rounded values :   0.2 fraction of incorrect :  0.217391304348\n",
        " For rounded values :   0.3 fraction of incorrect :  0.227272727273\n",
        " For rounded values :   0.4 fraction of incorrect :  0.227272727273\n",
        " For rounded values :   0.5 fraction of incorrect :  0.16\n",
        " For rounded values :   0.6 fraction of incorrect :  0.1\n",
        " For rounded values :   0.7 fraction of incorrect :  0.0\n",
        " For rounded values :   0.8 fraction of incorrect :  0.05\n",
        " For rounded values :   0.9 fraction of incorrect :  0.0769230769231\n",
        " For rounded values :   1.0 fraction of incorrect :  0.0\n",
        " For rounded values :   1.1 fraction of incorrect :  0.0\n",
        " For rounded values :   1.2 fraction of incorrect :  0.166666666667\n",
        " For rounded values :   1.3 fraction of incorrect :  0.0\n",
        " For rounded values :   1.4 fraction of incorrect :  0.0\n",
        " For rounded values :   1.5 fraction of incorrect :  0.0\n",
        " For rounded values :   1.6 fraction of incorrect :  0.0\n",
        " For rounded values :   1.7 fraction of incorrect :  0.0\n",
        " For rounded values :   2.1 fraction of incorrect :  0.0\n"
       ]
      }
     ],
     "prompt_number": 118
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## As we can see, the largest portion of error occurs when the predictions are near 0. \n",
      "## the farther away we get in either direction, the more confident we are in our result, and the less mistakes are made.\n",
      "\n",
      "plt.plot(res.keys(), res.values(), \"*\", color = 'red')\n",
      "plt.title(\"Test Error Vs. Confidence\")\n",
      "plt.xlabel('\"Confidence\" in prediction')\n",
      "plt.ylabel(\"Test error (0-1 loss)\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 133,
       "text": [
        "<matplotlib.text.Text at 0x11dbae890>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEZCAYAAABxbJkKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMxyzEnAzCzHnATMzHLs/wO+6w75zYCGGQAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1099bc210>"
       ]
      }
     ],
     "prompt_number": 133
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "5. Error Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Example 1:###\n",
      "#here is an example of a wrongly classified review:\n",
      "print \"classification : \", dotProduct(X_test_dict[1],w_buckets), \" True : \" , y_test[1]\n",
      "\n",
      "\n",
      "## To analyze contributin features we must:\n",
      "# get a list of features with corresponding \"feature strength\" for all features in w\n",
      "features = [(f ,w_buckets.get(f, 0) * v) for f, v in X_test_dict[1].items()]\n",
      "\n",
      "# sort them by the feature stength and print top 20: \n",
      "print sorted(features, key = lambda element : np.abs(element[1]))[-20:]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "classification :  0.137714159993  True :  -1\n",
        "[('about', 0.056481415618853996), ('no', 0.056667515835061098), ('point', 0.056853616051268201), ('least', 0.058063267456614387), ('shows', -0.058156317564717938), ('?', 0.058714618213339252), ('see', -0.059365968970064117), ('back', -0.060110369834892541), ('also', -0.061971371996963585), ('were', 0.063553223834723976), ('most', -0.063553223834723976), ('if', 0.0642976246995524), ('none', 0.068298779348005142), ('life', -0.072020783672147246), ('best', -0.073230435077493439), ('looks', 0.074533136590943155), ('have', 0.08011614307715631), ('seen', -0.081697994914916666), ('great', -0.086164400103887193), ('plot', 0.10654237377856322)]\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Example 2: ###\n",
      "print \"classification : \", dotProduct(X_test_dict[6],w_buckets), \" True : \" , y_test[6]\n",
      "# get a list of features with corresponding \"feature strength\" for all features in w\n",
      "features = [(f ,w_buckets.get(f, 0) * v) for f, v in X_test_dict[6].items()]\n",
      "\n",
      "# sort them by the feature stength and print top 20: \n",
      "print sorted(features, key = lambda element : np.abs(element[1]))[-20:]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "classification :  0.644837249158  True :  -1\n",
        "[('better', 0.056109215186439784), ('about', 0.056481415618853996), ('no', 0.056667515835061098), ('least', 0.058063267456614387), ('?', 0.058714618213339252), ('see', -0.059365968970064117), ('also', -0.061971371996963585), ('most', -0.063553223834723976), ('if', 0.0642976246995524), ('others', -0.065042025564380823), ('well', -0.067182178050762528), ('different', -0.06885707999662645), ('tries', 0.072486034212665015), ('does', -0.072951284753182744), ('looks', 0.074533136590943155), ('very', -0.075277537455771565), ('have', 0.08011614307715631), ('seen', -0.081697994914916666), ('unfortunately', 0.10942692712977303), ('bad', 0.16265158896499893)]\n"
       ]
      }
     ],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Example 2: ###\n",
      "print \"classification : \", dotProduct(X_test_dict[13],w_buckets), \" True : \" , y_test[13]\n",
      "# get a list of features with corresponding \"feature strength\" for all features in w\n",
      "features = [(f ,w_buckets.get(f, 0) * v) for f, v in X_test_dict[13].items()]\n",
      "\n",
      "# sort them by the feature stength and print top 20: \n",
      "print sorted(features, key = lambda element : np.abs(element[1]))[-20:]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "classification :  -0.0562953154026  True :  1\n",
        "[('?', 0.058714618213339252), ('see', -0.059365968970064117), ('then', 0.059645119294374778), ('back', -0.060110369834892541), ('first', -0.060296470051099643), ('simple', -0.061692221672652932), ('also', -0.061971371996963585), ('job', -0.062343572429377797), ('most', -0.063553223834723976), ('quite', -0.063832374159034644), ('if', 0.0642976246995524), ('well', -0.067182178050762528), ('does', -0.072951284753182744), ('town', -0.073230435077493425), ('yet', -0.074533136590943155), ('have', 0.08011614307715631), ('mess', 0.080674443725777617), ('fun', -0.09035165496854683), ('only', 0.093887559076481428), ('script', 0.096679062319587727)]\n"
       ]
      }
     ],
     "prompt_number": 168
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###The example above in an example of a negative review that was classified as positive.###\n",
      "#Examples 1,2 show negative reviews that was classified as positive\n",
      "#Example 3 shows a positive review that was classified as negative\n",
      "'''\n",
      "## Examining the most influential features we can see how that happened:\n",
      "in example 1:\n",
      "a) some of the most influential features are words like 'best', 'great', and 'plot' which we consider positive words.\n",
      "b) other features that contribute are ambiguous words like : \"none\" and \"least\" where it's unclear if it's positive or \n",
      "    negative. That is likely the result of the low confidence we have in our classification (y_predict = 0.1337)\n",
      "\n",
      "in example 2: \n",
      "a) some of the most influential features are words like 'better' , 'well' and 'most'.\n",
      "b) this is a somewhat surprising classification since some of our highest features are also include 'unfortunately' and 'bad'\n",
      "\n",
      "# interestingly enough, both examples that misclassify as negative as positive have '?' as a central feature. \n",
      "The impotance of such a nonsensical feature is something that we need to address and will do so later.\n",
      "\n",
      "in example 3:\n",
      "a) some of the most influential features are words like 'mess', 'quite', 'simple' which as all either negative \n",
      "or ambiguous words\".\n",
      "\n",
      "'''\n",
      "\n",
      "'''\n",
      "two features that might help fix these problems are:\n",
      "    1) n-grams: if we do not just use bag of words but also use tuples and 3-grams then we can retain more meaning!\n",
      "        for example, then \"plot\" in example 1 might be \"no plot\" \"better\" in example 2 might\n",
      "        become \"no better\" which is obviously negative.\"\n",
      "    2) It might be worthwhile to remove stopwords as well as regular expressions (such as the question mark we ran into earlier!)\n",
      "    3) add features for term frequency inverse document frequency (tf-idf). that way we can account for words that \n",
      "    are particularly relevant for a given review.\n",
      "        \n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#here is an example of a wrongly classified review:\n",
      "for i in range(10,20):\n",
      "    print i, \"classification : \", dotProduct(X_test_dict[i],w_buckets), \" True : \" , y_test[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 classification :  -0.863318902985  True :  -1\n",
        "11 classification :  -1.03639210406  True :  -1\n",
        "12 classification :  1.0069882699  True :  1\n",
        "13 classification :  -0.0562953154026  True :  1\n",
        "14 classification :  0.177818756586  True :  -1\n",
        "15 classification :  -0.33563173993  True :  -1\n",
        "16 classification :  0.0300551849174  True :  1\n",
        "17 classification :  0.3949977089  True :  -1\n",
        "18 classification :  0.462552087383  True :  1\n",
        "19 classification :  1.25291970561  True :  1\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "6. Features"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## The feature I chose to implement is tf-idf because I believed it would be the most effective.\n",
      "\n",
      "## first create a dictionary with document frequency based on trainning data\n",
      "Document_frequency = defaultdict(int) \n",
      "for i in range(len(X_train_dict)):\n",
      "    for word in X_train_dict[i].keys():\n",
      "        Document_frequency[word] += X_train_dict[i][word]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Create a training and testing set with the added features\n",
      "\n",
      "#trainning\n",
      "X_train_extra = []\n",
      "for i in range(len(X_train_dict)):\n",
      "    dict_extra_features = defaultdict(float) \n",
      "    \n",
      "    for word in X_train_dict[i].keys():\n",
      "        if Document_frequency[word]:\n",
      "            dict_extra_features['%s_tfidf' %word] = float(X_train_dict[i][word])/Document_frequency[word]\n",
      "        dict_extra_features[word] = X_train_dict[i][word]\n",
      "    X_train_extra.append(dict_extra_features)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# testing \n",
      "X_test_extra = []\n",
      "for r in X_test_dict:\n",
      "    dict_extra_features = defaultdict(float) \n",
      "    for word in r.keys():\n",
      "        if Document_frequency[word]:\n",
      "            dict_extra_features['%s_tfidf' %word] = float(r[word])/Document_frequency[word]\n",
      "        dict_extra_features[word] = r[word]\n",
      "\n",
      "    X_test_extra.append(dict_extra_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Use previous successful range of lambdas to examine change \n",
      "Lambda_opt = optimize_lambda(X_train_extra,y_train, X_test_extra, y_test, Lambda_range = Lambda_range2)\n",
      "print Lambda_opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lambda:  0.01\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.424347811302\n",
        "zero-one loss :  0.158\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.231111111111\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.511195052028\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.452222222222\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.559630422562\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.673333333333\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.611025204839\n",
        "zero-one loss :  0.152\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.894444444444\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.653213204333\n",
        "zero-one loss :  0.156\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.11555555556\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.689707844871\n",
        "zero-one loss :  0.162\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.33666666667\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.722828327303\n",
        "zero-one loss :  0.178\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.55777777778\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.752055180238\n",
        "zero-one loss :  0.212\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.77888888889\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.776285230681\n",
        "zero-one loss :  0.222\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.0\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.798698293456\n",
        "zero-one loss :  0.242\n",
        "************\n",
        "0.231111111111\n"
       ]
      }
     ],
     "prompt_number": 242
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## As we can see by examining previous results, our best Lambda remains the same but we actually do WORSE then previously. \n",
      "## To make sure that this is simply due to having a lambda range that is not fine grained enough, \n",
      "## I ran analysis on a more fine grained lambda range:"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 246
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Results with extra features\n",
      "\n",
      "Lambda_range3 = np.linspace(0.01, 0.5, 10 )\n",
      "Lambda_opt = optimize_lambda(X_train_extra,y_train, X_test_extra, y_test, Lambda_range = Lambda_range3)\n",
      "print Lambda_opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lambda:  0.01\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.424347811302\n",
        "zero-one loss :  0.158\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0644444444444\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.479600001068\n",
        "zero-one loss :  0.162\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.118888888889\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.491246238854\n",
        "zero-one loss :  0.16\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.173333333333\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.499858438407\n",
        "zero-one loss :  0.148\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.227777777778\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.510758602781\n",
        "zero-one loss :  0.15\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.282222222222\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.521894660706\n",
        "zero-one loss :  0.142\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.336666666667\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.53353637134\n",
        "zero-one loss :  0.144\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.391111111111\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.545466184973\n",
        "zero-one loss :  0.144\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.445555555556\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.558200674713\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.571441334284\n",
        "zero-one loss :  0.148\n",
        "************\n",
        "0.282222222222\n"
       ]
      }
     ],
     "prompt_number": 243
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Results with no added features\n",
      "Lambda_range3 = np.linspace(0.01, 0.5, 10 )\n",
      "Lambda_opt = optimize_lambda(X_train_dict,y_train, X_test_dict, y_test, Lambda_range = Lambda_range3)\n",
      "print Lambda_opt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Lambda:  0.01\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.421371239328\n",
        "zero-one loss :  0.168\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0644444444444\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.472123289896\n",
        "zero-one loss :  0.164\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.118888888889\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.483350659327\n",
        "zero-one loss :  0.162\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.173333333333\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.494191582315\n",
        "zero-one loss :  0.148\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.227777777778\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.505352525857\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.282222222222\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.517959091219\n",
        "zero-one loss :  0.142\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.336666666667\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.530203915555\n",
        "zero-one loss :  0.144\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.391111111111\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.542894062884\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.445555555556\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.555644708898\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "Lambda: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5\n",
        "loss :  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.569487537902\n",
        "zero-one loss :  0.146\n",
        "************\n",
        "0.282222222222\n"
       ]
      }
     ],
     "prompt_number": 244
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Again we seem to get similar results with and without the extra features.\n",
      "## To see if perhaps the tf-idf just aren't proving to be relevant features, \n",
      "## let's perform the same error analysis we've done previously on an example that previously a misclassification: (Example 3)\n",
      "\n",
      "\n",
      "## Interestingly enough , the tf-idf features aren't proving to be important features. \n",
      "## They are assigne weights even with regularization (I checked). However the weights are not significant enough to make top 20.\n",
      "## It is possible that this is happening because our training set is too small, which causes words to appear \"rarer\" than they \n",
      "## actually are. If that were to happen then the tf-idf would be effective. My next feature to try would definitely be 2-grams\n",
      "\n",
      "## As a side note: the assignment asked us to look at the sample variance, but since the results are near identical \n",
      "## with and without the added features, there was no value in analyzing the variance. "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w_buckets = Vectorized_Pegasos_algorithm(X_train_extra, y_train, Lambda=0.282222222222, max_epochs = 30) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 247
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"classification : \", dotProduct(X_test_extra[13],w_buckets), \" True : \" , y_test[13]\n",
      "# get a list of features with corresponding \"feature strength\" for all features in w\n",
      "features = [(f ,w_buckets.get(f, 0) * v) for f, v in X_test_extra[13].items()]\n",
      "\n",
      "# sort them by the feature stength and print top 20: \n",
      "print sorted(features, key = lambda element : np.abs(element[1]))[-20:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "classification :  0.0119144657558  True :  1\n",
        "[('no', 0.05547251799008958), ('simple', -0.05577731204498019), ('then', 0.05608210609987079), ('first', -0.056386900154761395), ('see', -0.0579108704292144), ('job', -0.058215664484105006), ('most', -0.06019682584089392), ('?', 0.06103500949184309), ('well', -0.06118740651928836), ('also', -0.0625589797662961), ('quite', -0.06286377382118669), ('if', 0.06324476638979995), ('town', -0.06431154558191705), ('does', -0.06774047869943635), ('yet', -0.06880725789155345), ('mess', 0.0723123895227954), ('have', 0.07513173453053347), ('fun', -0.08077042454600962), ('script', 0.08930465808294634), ('only', 0.0896094521378369)]\n"
       ]
      }
     ],
     "prompt_number": 248
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}